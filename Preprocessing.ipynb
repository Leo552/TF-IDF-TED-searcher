{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the file path will be different for you.\n",
    "\n",
    "The dataset can be downloaded at https://data.world/owentemple/ted-talks-complete-list. I removed a the number of additional features in the dataset with the transcription since I won't be using them with TF-IDF. See the column headings below to see which ones I have kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and remove null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_vals(df):\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def load_data(path):\n",
    "    # Will infer the headings of each column\n",
    "    dataframe = pd.read_csv(path)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>URL</th>\n",
       "      <th>description</th>\n",
       "      <th>year_filmed</th>\n",
       "      <th>duration</th>\n",
       "      <th>views_as_of_06162017</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>http://www.ted.com/talks/view/id/1</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:16:17</td>\n",
       "      <td>3177001.0</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>http://www.ted.com/talks/view/id/2</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:15:06</td>\n",
       "      <td>1379328.0</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>http://www.ted.com/talks/view/id/3</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>2005</td>\n",
       "      <td>00:18:45</td>\n",
       "      <td>790536.0</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>0:12\\r\\r\\rA public, Dewey long ago observed,\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>http://www.ted.com/talks/view/id/4</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:19:37</td>\n",
       "      <td>1985119.0</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>0:11\\r\\r\\rI want to start off by saying, Houst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>http://www.ted.com/talks/view/id/5</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>2002</td>\n",
       "      <td>00:20:04</td>\n",
       "      <td>859487.0</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>0:12\\r\\r\\rWhat I want to talk about is, as bac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       speaker                              headline  \\\n",
       "0   1       Al Gore           Averting the climate crisis   \n",
       "1   2     Amy Smith         Simple designs to save a life   \n",
       "2   3  Ashraf Ghani         How to rebuild a broken state   \n",
       "3   4    Burt Rutan  The real future of space exploration   \n",
       "4   5  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                  URL  \\\n",
       "0  http://www.ted.com/talks/view/id/1   \n",
       "1  http://www.ted.com/talks/view/id/2   \n",
       "2  http://www.ted.com/talks/view/id/3   \n",
       "3  http://www.ted.com/talks/view/id/4   \n",
       "4  http://www.ted.com/talks/view/id/5   \n",
       "\n",
       "                                         description  year_filmed  duration  \\\n",
       "0  With the same humor and humanity he exuded in ...         2006  00:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...         2006  00:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...         2005  00:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...         2006  00:19:37   \n",
       "4  American designer Chris Bangle explains his ph...         2002  00:20:04   \n",
       "\n",
       "   views_as_of_06162017                                               tags  \\\n",
       "0             3177001.0  cars,alternative energy,culture,politics,scien...   \n",
       "1             1379328.0  MacArthur grant,simplicity,industrial design,a...   \n",
       "2              790536.0  corruption,poverty,economics,investment,milita...   \n",
       "3             1985119.0  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4              859487.0  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript  \n",
       "0  0:14\\r\\r\\rThank you so much, Chris.\\rAnd it's ...  \n",
       "1  0:11\\r\\r\\rIn terms of invention,\\rI'd like to ...  \n",
       "2  0:12\\r\\r\\rA public, Dewey long ago observed,\\r...  \n",
       "3  0:11\\r\\r\\rI want to start off by saying, Houst...  \n",
       "4  0:12\\r\\r\\rWhat I want to talk about is, as bac...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'C:\\\\Users\\\\maxel\\\\OneDrive\\\\Search_Engine\\\\Version_1\\\\TED_Talks_dataset.csv'\n",
    "df = load_data(PATH)\n",
    "df = remove_null_vals(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that we will consider is the decription, tags, heading and transcription. However we must account for the heading, description and tags to carry more weight per word when processing the search query since they give a better description of what the talk will be about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the complexity of the search, we shall make a new feature which combines the description, tags and the title to make a more highly weighted description of the talk compared to the transcripted (in terms of description the individual words carry). We shall call this feature exposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tags_description_heading(df):\n",
    "    s = ' '\n",
    "    df['exposition'] = [row['headline'] +s+ row['description'] +s+ row['tags'] for index, row in df.iterrows()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Averting the climate crisis With the same humor and humanity he exuded in \"An Inconvenient Truth,\" Al Gore spells out 15 ways that individuals can address climate change immediately, from buying a hybrid to inventing a new, hotter brand name for global warming. cars,alternative energy,culture,politics,science,climate change,environment,sustainability,global issues,technology'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = combine_tags_description_heading(df)\n",
    "df['exposition'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added a space when combining the headline, description and tags to ensure the words are spilt when tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must be pre-processed in order to ensure we achieve maximum accuracy with the search. We shall conduct stemming and lemmetisation on the the transcripts, decription and heading of the talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    text = re.split(r'[;,.\\s]\\s*', text)\n",
    "    stop_words = stopwords.words('english')\n",
    "    index_to_remove = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        \n",
    "        # Make the word lower case\n",
    "        text[i] = text[i].lower()\n",
    "        \n",
    "        # Remove the word if the word is a stop word\n",
    "        if text[i] in stop_words:\n",
    "            index_to_remove.append(i)\n",
    "            continue\n",
    "\n",
    "    for index in index_to_remove[::-1]:\n",
    "        text.pop(index)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_symbols(text):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\\r0123456789\"\n",
    "    for symbol in symbols:\n",
    "        text = text.replace(symbol, ' ')\n",
    "        \n",
    "    return text\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    return [word.replace('`', '') for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must be careful about certain words such as the conversion of U.S to us or apostrophes. \"Won't\" will be converted to \"wont\" so won't be removed with the stop words. We must take out the apstrophes separately afterwards.\n",
    "\n",
    "We are also going to remove numbers since the transcript files contain timings which are not relavent to the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming converts words to its stem. For example running and ran is converted to run based on some set of rules. This is what we want since it does really make a difference which tense the word is in for our search query. We are going to use a library for this because coding a stemmer seems like it would be boring and not much would be gained from it. The library will be Porter-Stemmer which identifies and removes the suffix or affix of a word (the attachments on the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run play undecid\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'), stemmer.stem('played'), stemmer.stem('undecided')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmetisation is reducing a word to its root synonym. Unlike stemming, lemmatisation will produce a word that is in a set dictionary. We shall use stemming for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        text[i] = stemmer.stem(text[i])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together in a function we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    for target_text in ['exposition', 'transcript']:\n",
    "        for index, row in df.iterrows():\n",
    "            text = row[target_text]\n",
    "            \n",
    "            text = remove_symbols(text)\n",
    "            text = remove_stop_words(text)\n",
    "            text = remove_apostrophes(text)\n",
    "            text = stemming(text)\n",
    "            \n",
    "            df[target_text][index] = text\n",
    "    \n",
    "    return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May take a while to run depending on the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxel\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>headline</th>\n",
       "      <th>URL</th>\n",
       "      <th>description</th>\n",
       "      <th>year_filmed</th>\n",
       "      <th>duration</th>\n",
       "      <th>views_as_of_06162017</th>\n",
       "      <th>tags</th>\n",
       "      <th>transcript</th>\n",
       "      <th>exposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>http://www.ted.com/talks/view/id/1</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:16:17</td>\n",
       "      <td>3177001.0</td>\n",
       "      <td>cars,alternative energy,culture,politics,scien...</td>\n",
       "      <td>[, thank, much, chri, truli, great, honor, opp...</td>\n",
       "      <td>[avert, climat, crisi, humor, human, exud, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Amy Smith</td>\n",
       "      <td>Simple designs to save a life</td>\n",
       "      <td>http://www.ted.com/talks/view/id/2</td>\n",
       "      <td>Fumes from indoor cooking fires kill more than...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:15:06</td>\n",
       "      <td>1379328.0</td>\n",
       "      <td>MacArthur grant,simplicity,industrial design,a...</td>\n",
       "      <td>[, term, invent, i'd, like, tell, tale, one, f...</td>\n",
       "      <td>[simpl, design, save, life, fume, indoor, cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ashraf Ghani</td>\n",
       "      <td>How to rebuild a broken state</td>\n",
       "      <td>http://www.ted.com/talks/view/id/3</td>\n",
       "      <td>Ashraf Ghani's passionate and powerful 10-minu...</td>\n",
       "      <td>2005</td>\n",
       "      <td>00:18:45</td>\n",
       "      <td>790536.0</td>\n",
       "      <td>corruption,poverty,economics,investment,milita...</td>\n",
       "      <td>[, public, dewey, long, ago, observ, constitut...</td>\n",
       "      <td>[rebuild, broken, state, ashraf, ghani', passi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Burt Rutan</td>\n",
       "      <td>The real future of space exploration</td>\n",
       "      <td>http://www.ted.com/talks/view/id/4</td>\n",
       "      <td>In this passionate talk, legendary spacecraft ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>00:19:37</td>\n",
       "      <td>1985119.0</td>\n",
       "      <td>aircraft,flight,industrial design,NASA,rocket ...</td>\n",
       "      <td>[, want, start, say, houston, problem, we'r, e...</td>\n",
       "      <td>[real, futur, space, explor, passion, talk, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Chris Bangle</td>\n",
       "      <td>Great cars are great art</td>\n",
       "      <td>http://www.ted.com/talks/view/id/5</td>\n",
       "      <td>American designer Chris Bangle explains his ph...</td>\n",
       "      <td>2002</td>\n",
       "      <td>00:20:04</td>\n",
       "      <td>859487.0</td>\n",
       "      <td>cars,industrial design,transportation,inventio...</td>\n",
       "      <td>[, want, talk, background, idea, car, art, act...</td>\n",
       "      <td>[great, car, great, art, american, design, chr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       speaker                              headline  \\\n",
       "0   1       Al Gore           Averting the climate crisis   \n",
       "1   2     Amy Smith         Simple designs to save a life   \n",
       "2   3  Ashraf Ghani         How to rebuild a broken state   \n",
       "3   4    Burt Rutan  The real future of space exploration   \n",
       "4   5  Chris Bangle              Great cars are great art   \n",
       "\n",
       "                                  URL  \\\n",
       "0  http://www.ted.com/talks/view/id/1   \n",
       "1  http://www.ted.com/talks/view/id/2   \n",
       "2  http://www.ted.com/talks/view/id/3   \n",
       "3  http://www.ted.com/talks/view/id/4   \n",
       "4  http://www.ted.com/talks/view/id/5   \n",
       "\n",
       "                                         description  year_filmed  duration  \\\n",
       "0  With the same humor and humanity he exuded in ...         2006  00:16:17   \n",
       "1  Fumes from indoor cooking fires kill more than...         2006  00:15:06   \n",
       "2  Ashraf Ghani's passionate and powerful 10-minu...         2005  00:18:45   \n",
       "3  In this passionate talk, legendary spacecraft ...         2006  00:19:37   \n",
       "4  American designer Chris Bangle explains his ph...         2002  00:20:04   \n",
       "\n",
       "   views_as_of_06162017                                               tags  \\\n",
       "0             3177001.0  cars,alternative energy,culture,politics,scien...   \n",
       "1             1379328.0  MacArthur grant,simplicity,industrial design,a...   \n",
       "2              790536.0  corruption,poverty,economics,investment,milita...   \n",
       "3             1985119.0  aircraft,flight,industrial design,NASA,rocket ...   \n",
       "4              859487.0  cars,industrial design,transportation,inventio...   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  [, thank, much, chri, truli, great, honor, opp...   \n",
       "1  [, term, invent, i'd, like, tell, tale, one, f...   \n",
       "2  [, public, dewey, long, ago, observ, constitut...   \n",
       "3  [, want, start, say, houston, problem, we'r, e...   \n",
       "4  [, want, talk, background, idea, car, art, act...   \n",
       "\n",
       "                                          exposition  \n",
       "0  [avert, climat, crisi, humor, human, exud, inc...  \n",
       "1  [simpl, design, save, life, fume, indoor, cook...  \n",
       "2  [rebuild, broken, state, ashraf, ghani', passi...  \n",
       "3  [real, futur, space, explor, passion, talk, le...  \n",
       "4  [great, car, great, art, american, design, chr...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to run this every time we want to calculate TF-IDF so we will save if to a csv file after converting the exposition and transcription into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['exposition'] = [','.join(row['exposition']) for i, row in df.iterrows()]\n",
    "\n",
    "df['transcript'] = [','.join(row['transcript']) for i, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'processed_TED_Talks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-380bb218bd31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'processed_TED_Talks.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Machine_Learning\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'processed_TED_Talks.csv'"
     ]
    }
   ],
   "source": [
    "df.to_csv('processed_TED_Talks.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
