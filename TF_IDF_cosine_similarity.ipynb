{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numpy arrays for faster access and pandas to make use of the dataframe class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    dataframe = pd.read_csv(path)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\maxel\\\\OneDrive\\\\Search_Engine\\\\Version_1\\\\processed_TED_Talks.csv'\n",
    "df_info = load_data(PATH)\n",
    "N = len(df_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_exposition = np.array([np.array(row['exposition'][1:].split(',')) for i, row in df_info.iterrows()])\n",
    "processed_transcript = np.array([np.array(row['transcript'][1:].split(',')) for i, row in df_info.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the processed expostition and transcript into a numpy array for a faster speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating DF for both the exposition and and the transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF - document frequency - how many documents the word appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_exposition[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i} # Set type\n",
    "\n",
    "    tokens = processed_transcript[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i} # Set type\n",
    "            \n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i]) # Get the number of documents the token appeared in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40737"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab_size = len(DF)\n",
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = list(DF.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that will return the document frequency of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DF(word):\n",
    "    try:\n",
    "        return DF[word]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/term_frequency.png' height=\"30\" width = \"300\">\n",
    "<img src='Images/normalised_term_frequency.png' height=\"30\" width = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    " - tf is the term frequency\n",
    " - f is the frequency of a given word\n",
    " - d is the given document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency is given by the equation above. The choice can be made as to which equation is used. The first equation is simply a fraction of the words that is the token. The second gives a logarithmic way of describing the frequency of the words. We shall be using the logarithmic equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/inverse_document_frequency.png' height=\"60\" width = \"350\">\n",
    "<img src='Images/inverse_document_frequency_smooth.png'  height=\"60\" width = \"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    " - idf is the inverse document frequency\n",
    " - N is the number of documents\n",
    " - n is the document frequency of the given token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also choice in the way that inverse document frequency is calcuated using the equations above. Jusitification for the use of this have included attempts to connect it to Zipf's law https://en.wikipedia.org/wiki/Zipf%27s_law, information theory or a probability. See more here https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Justification_of_idf.\n",
    "\n",
    "We shall be using the smooth idf equation. The use of the plus one to the denominator is so that an undefined error does not occur. 1 will also be added to N to ensure that we don't have to compute the log of 0 (although this is not written in the equation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/tfidf.png' height=\"60\" width = \"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a function which will calculate the tf-idf weights of each of the documens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the TF-IDF weights separately for the exposition and the transcript then merge then according to some given weight. The weight (called alpha) will try best describe how much more useful the exposition is at descibing the talk compared to the transcription per word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating TF-IDF for transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_transcript[i]\n",
    "    \n",
    "    counter = Counter(list(tokens) + list(processed_exposition[i]))\n",
    "    words_count = len(tokens) + len(processed_exposition[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = get_DF(token)\n",
    "        idf = 1 + np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[i, token] = tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the TF-IDF for the exposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_expo = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_exposition[i]\n",
    "    \n",
    "    counter = Counter(list(tokens) + list(processed_transcript[i]))\n",
    "    words_count = len(tokens) + len(processed_transcript[i])\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = get_DF(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_expo[i, token] = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2378347668491331"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf[(1948,\"quantum\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1876029275910187"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_expo[(1948,\"quantum\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086962"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the TF-IDF according to the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge them into one weight matrix according to the alpha weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/tf_idf_merge.png' height=\"60\" width = \"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tf_idf:\n",
    "    tf_idf[i] = (1 - alpha) * tf_idf[i]\n",
    "    \n",
    "    try:\n",
    "        tf_idf[i] += alpha * tf_idf_expo[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching score simularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the preprocess import is for a python script which simply contains the preprocessing functions written earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching score is simply how close the search query is in terms of its tfidf weights. Think of it as how close two points are in 2d or 3d space, you may struggle imagining a 40 thousand dimension space. The program below uses Manhatten distance (the blocky distance) where rather than drawing a straight line between the points, lines are drawn parallel to the axis and the sum of these distances are used. You can think of this as going along the edges of a cube rather than cutting throught the middle to get between opposite corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    tokens = preprocessed_query\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"Search Query:\", query, '\\n')\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "        print(i[0], df_info['headline'][i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Score\n",
      "Search Query: Quantum biology \n",
      "\n",
      "1948 How quantum biology might explain lifeâ€™s biggest questions\n",
      "1198 The levitating superconductor\n",
      "912 Making sense of a visible quantum object\n",
      "970 Making matter come alive\n",
      "706 The bio-future of joint replacement\n",
      "1347 Print your own medicine\n",
      "1159 What's left to explore?\n",
      "2155 Clues to prehistoric times, found in blind cavefish\n",
      "1124 How can technology transform the human body?\n",
      "166 Using biology to rethink the energy challenge\n"
     ]
    }
   ],
   "source": [
    "matching_score(10, \"Quantum biology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem to be giving the right answers. But we can improve this even more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Cosine Similarity Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/scalar_product.png' height=\"60\" width = \"400\">\n",
    "<img src='Images/angle_between_vectors.png' height=\"60\" width = \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation above can be used to find the angle between two vectors. This can be used to find the angle between the search query and the document vectors. From this we can deduce that the smaller the angle the better the query matches the document. Cosine similarity is used when the magnitude of the vector doesn't matter. This is so since we are don't mind how long the talks are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/cosine_sim_graph.png' width = \"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(tokens):\n",
    "    \n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = get_DF(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query):\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    tokens = preprocessed_query\n",
    "    \n",
    "    print(\"Search Query:\", query, '\\n')\n",
    "    \n",
    "    d_cosines = []\n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    for i in out[:k]:\n",
    "        print(i, df_info['headline'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "Search Query: Groundwater \n",
      "\n",
      "2031 4 ways we can avoid a catastrophic drought\n",
      "2035 The mysterious world of underwater caves\n",
      "173 Why aren't we more compassionate?\n",
      "1734 An engineer's vision for tiny forests, everywhere\n",
      "940 Let's take back the Internet!\n",
      "2178 The taboo secret to better health\n",
      "2155 Clues to prehistoric times, found in blind cavefish\n",
      "554 The ancient ingenuity of water harvesting\n",
      "2020 What happens when a city runs out of room for its dead\n",
      "2105 Hunting for dinosaurs showed me our place in the universe\n"
     ]
    }
   ],
   "source": [
    "Q = cosine_similarity(10, \"Groundwater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to be computing the TF-IDF matrix every time we want to search something so we will save it to a file (although this process itself does take some time to load up again). We will use the npy file extension which has faster read times than a plain text or csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a file\n",
    "np.save('tf_idf_dict.npy', tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_tf_idf = np.load('tf_idf_dict.npy', allow_pickle = True).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
